---
title: 'An instruction set for ANNs -- building a VM for virtual machines'
description: 'An of the cuff thought I had'
pubDate: 'Jan 20 2024'
heroImage: '/PersonalBlog/nn.png'
---
import Latex from '../../components/Latex.astro';
import { Code } from 'astro:components';


<i>To preface this article, I have little-to-no professional experience with machine learning, so take
this article's with the smallest grain of salt. I have some background with college, internships,
and casual reading through out the years, I still have a basic understanding, at best -- though,
that won't stop me from writing this article. I thought this blog would be a good mental exercise,
as I don't get a chance to explore random ideas I have since graduatin.  I have no interest nor the
skill to have some revolutionary, useful insight; I simply want to spitball and theorycraft
through fun ideas without thinking of practical reasons why not to and why the idea is uninformed</i>

## Introduction

I was reading some opinion's regarding the future of Artificial-intelligence, given the
extraordinary capabilities of LLM and how quickly they became ever-present. They were dicussing
the cap in the growth of these models' 'intelligence.' Generally, there are two potential limits I've seen
brought up: the number of parameters (more generally the transformer architecture)  and the quantity of data (
there is an overall limit to the amount of data these LLMs can be trained on.) The first limit is what
caught my eye.

Honestly, I forgot most of what I read, but the general takeaway was that scaling based on parameters wasn't
tenable. Either there would be architectural overhauls -- moving away from backpropogation for instance, hardware
improvements or incremental software improvements. And quite frankly that makes sense to me; without getting into specific,
to improve a generic ml model we can either a) use more and better data b) a better logical architecuture for training or
representing our model c) train a 'deeper' model which to be done in a reasonable time needs more compute, either in quantity
or quality of hardware.

Whenever, I've read up anologies between the brain and ANN, I'm immediately bombarded with groans about how they aren't analogus,
and we shouldnt seriously be trying draw parallels. But I was wondering why the learning of brains isn't completely
dwarfed by that of a computer-based ANN. I'm choosing to discount the duration of learning, which for a brain spans over decades,
because I believe that is offset by the scale of hardware and the 'intentional training' of the models vs mostly wasted time of
learning for human brains. Furthermore, the number of parameters for a brain vs an ANN is roughly similar in magnitude (80 billion to
300-1000 billion for Chatgpt), but that gaps probably widens significantly when we can consider a substantial amount of the brain's
compute goes towards subconcious physical processes (eg. motor skills, breathing etc). Its important to note I also know nothing about
Biology.

One thing I thought is that we rely alot upon abstractions for computation in the ANN. While abstractions are useful because
it simplifies the logical problems we are facing, they can limit our ability to reason about the exact computational steps we make.
The uniformed thought/hypthesis I had is that there a wastage due to the brain more 'general-purpose' than a computer which has a lot
more competing factors for a running program.

## Idea
export const f1 = 'Y = f(w^{T}\\vec{x} + \\vec{b})';
export const f2 = '\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}';

Starting with the simplest possible function: 

Let <Latex formula='w,x \in \R^n'/>, <Latex formula='b \in \R'/>, <Latex formula='f:R \to R \in C^{-1}(\R)'/>. Technically, this is over
numbers representable by floating point and not R, but this representation is close enough

<Latex formula={f1}/>

We can represent this function as an instruction <Latex formula='dot\space a \space b \space c'/>, which takes two registers a and b
and performs a dot product on them and stores the result into c.

```cpp
int dot(int* v1, int* v2, int num) {
    int total = 0;
    for (int i = 0; i < num; i++) {
        total += v1[i] * v2[i];
    }
}
```

```asm
dot(int*, int*, int):
        sub     sp, sp, #48
        str     x0, [sp, 24]
        str     x1, [sp, 16]
        str     w2, [sp, 12]
        str     wzr, [sp, 44]
        str     wzr, [sp, 40]
        b       .L2
.L3:
        ldrsw   x0, [sp, 40]
        lsl     x0, x0, 2
        ldr     x1, [sp, 24]
        add     x0, x1, x0
        ldr     w1, [x0]
        ldrsw   x0, [sp, 40]
        lsl     x0, x0, 2
        ldr     x2, [sp, 16]
        add     x0, x2, x0
        ldr     w0, [x0]
        mul     w0, w1, w0
        ldr     w1, [sp, 44]
        add     w0, w1, w0
        str     w0, [sp, 44]
        ldr     w0, [sp, 40]
        add     w0, w0, 1
        str     w0, [sp, 40]
.L2:
        ldr     w1, [sp, 40]
        ldr     w0, [sp, 12]
        cmp     w1, w0
        blt     .L3
        brk #1000
```

In theory, this would be carried out by a single instruction <Latex formula='dot\space a \space b \space c'/>. However, in practice, it seems
difficult to imagine that being an atomic instruction that the CPU could carry out.